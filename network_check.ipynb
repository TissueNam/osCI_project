{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vector'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-980a84d3439f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclamp_to_unit_sphere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vector'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from vector import clamp_to_unit_sphere\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # TODO: what about fully-connected layers?\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.05)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class encoder32(nn.Module):\n",
    "    def __init__(self, latent_size=100, num_classes=2, batch_size=64, **kwargs):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,       64,     3, 1, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64,      64,     3, 1, 1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64,     128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv5 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv6 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "        # Shortcut out of the network at 8x8\n",
    "        self.conv_out_6 = nn.Conv2d(128, latent_size, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv8 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv9 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "        # Shortcut out of the network at 4x4\n",
    "        self.conv_out_9 = nn.Conv2d(128, latent_size, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "        self.conv_out_10 = nn.Conv2d(128, latent_size, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        self.bn9 = nn.BatchNorm2d(128)\n",
    "        self.bn10 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(128*2*2, latent_size)\n",
    "\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "        self.dr2 = nn.Dropout2d(0.2)\n",
    "        self.dr3 = nn.Dropout2d(0.2)\n",
    "        self.dr4 = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.cuda()\n",
    "\n",
    "    def forward(self, x, output_scale=1):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        x = self.dr1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = self.dr2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        # Image representation is now 8 x 8\n",
    "        if output_scale == 8:\n",
    "            x = self.conv_out_6(x)\n",
    "            x = x.view(batch_size, -1)\n",
    "            x = clamp_to_unit_sphere(x, 8*8)\n",
    "            return x\n",
    "\n",
    "        x = self.dr3(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.bn9(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        # Image representation is now 4x4\n",
    "        if output_scale == 4:\n",
    "            x = self.conv_out_9(x)\n",
    "            x = x.view(batch_size, -1)\n",
    "            x = clamp_to_unit_sphere(x, 4*4)\n",
    "            return x\n",
    "\n",
    "        x = self.dr4(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.bn10(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        # Image representation is now 2x2\n",
    "        if output_scale == 2:\n",
    "            x = self.conv_out_10(x)\n",
    "            x = x.view(batch_size, -1)\n",
    "            x = clamp_to_unit_sphere(x, 2*2)\n",
    "            return x\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc1(x)\n",
    "        x = clamp_to_unit_sphere(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class generator32(nn.Module):\n",
    "    def __init__(self, latent_size=100, batch_size=64, **kwargs):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.fc1 = nn.Linear(latent_size, 512*2*2, bias=False)\n",
    "\n",
    "        self.conv2_in = nn.ConvTranspose2d(latent_size, 512, 1, stride=1, padding=0, bias=False)\n",
    "        self.conv2 = nn.ConvTranspose2d(   512,      512, 4, stride=2, padding=1, bias=False)\n",
    "        self.conv3_in = nn.ConvTranspose2d(latent_size, 512, 1, stride=1, padding=0, bias=False)\n",
    "        self.conv3 = nn.ConvTranspose2d(   512,      256, 4, stride=2, padding=1, bias=False)\n",
    "        self.conv4_in = nn.ConvTranspose2d(latent_size, 256, 1, stride=1, padding=0, bias=False)\n",
    "        self.conv4 = nn.ConvTranspose2d(   256,      128, 4, stride=2, padding=1, bias=False)\n",
    "        self.conv5 = nn.ConvTranspose2d(   128,        3, 4, stride=2, padding=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.apply(weights_init)\n",
    "        self.cuda()\n",
    "\n",
    "    def forward(self, x, input_scale=1):\n",
    "        batch_size = x.shape[0]\n",
    "        if input_scale <= 1:\n",
    "            x = self.fc1(x)\n",
    "            x = x.resize(batch_size, 512, 2, 2)\n",
    "\n",
    "        # 512 x 2 x 2\n",
    "        if input_scale == 2:\n",
    "            x = x.view(batch_size, self.latent_size, 2, 2)\n",
    "            x = self.conv2_in(x)\n",
    "        if input_scale <= 2:\n",
    "            x = self.conv2(x)\n",
    "            x = nn.LeakyReLU()(x)\n",
    "            x = self.bn2(x)\n",
    "\n",
    "        # 512 x 4 x 4\n",
    "        if input_scale == 4:\n",
    "            x = x.view(batch_size, self.latent_size, 4, 4)\n",
    "            x = self.conv3_in(x)\n",
    "        if input_scale <= 4:\n",
    "            x = self.conv3(x)\n",
    "            x = nn.LeakyReLU()(x)\n",
    "            x = self.bn3(x)\n",
    "\n",
    "        # 256 x 8 x 8\n",
    "        if input_scale == 8:\n",
    "            x = x.view(batch_size, self.latent_size, 8, 8)\n",
    "            x = self.conv4_in(x)\n",
    "        if input_scale <= 8:\n",
    "            x = self.conv4(x)\n",
    "            x = nn.LeakyReLU()(x)\n",
    "            x = self.bn4(x)\n",
    "        # 128 x 16 x 16\n",
    "        x = self.conv5(x)\n",
    "        # 3 x 32 x 32\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class multiclassDiscriminator32(nn.Module):\n",
    "    def __init__(self, latent_size=100, num_classes=2, batch_size=64, **kwargs):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = nn.Conv2d(3,       64,     3, 1, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64,      64,     3, 1, 1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64,     128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv5 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv6 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv8 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv9 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        self.bn9 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(128*4*4 * 2, num_classes)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "        self.dr2 = nn.Dropout2d(0.2)\n",
    "        self.dr3 = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.cuda()\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        x = self.dr1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = self.dr2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = self.dr3(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.bn9(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "        if return_features:\n",
    "            return x\n",
    "\n",
    "        # Lazy minibatch discrimination: avg of other examples' features\n",
    "        batch_avg = torch.exp(-x.mean(dim=0))\n",
    "        batch_avg = batch_avg.expand(batch_size, -1)\n",
    "        x = torch.cat([x, batch_avg], dim=1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class classifier32(nn.Module):\n",
    "    def __init__(self, latent_size=100, num_classes=2, batch_size=64, **kwargs):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = nn.Conv2d(3,       64,     3, 1, 1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64,      64,     3, 1, 1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(64,     128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv5 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv6 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv8 = nn.Conv2d(128,    128,     3, 1, 1, bias=False)\n",
    "        self.conv9 = nn.Conv2d(128,    128,     3, 2, 1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn7 = nn.BatchNorm2d(128)\n",
    "        self.bn8 = nn.BatchNorm2d(128)\n",
    "        self.bn9 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.fc1 = nn.Linear(128*4*4, num_classes)\n",
    "        self.dr1 = nn.Dropout2d(0.2)\n",
    "        self.dr2 = nn.Dropout2d(0.2)\n",
    "        self.dr3 = nn.Dropout2d(0.2)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.cuda()\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        x = self.dr1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = self.dr2(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = self.dr3(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.bn8(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.conv9(x)\n",
    "        x = self.bn9(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "        if return_features:\n",
    "            return x\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd0f443c155abb0b2f77c12ee18232ee239923b5073f954626b927b44a2c2cc8262",
   "display_name": "Python 3.6  ('osCI3': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "f443c155abb0b2f77c12ee18232ee239923b5073f954626b927b44a2c2cc8262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}